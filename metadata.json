{
  "name": "Copy of Viralyzaier: Your AI Video Growth Engine.",
  "description": "Viralyzaier is the end-to-end AI strategic partner for experts, coaches, and educators who want to grow their business with video. It transforms your expertise into high-performing content through a guided workflow that covers scriptwriting, asset generation, performance analysis, and data-driven growth. Stop guessing and start building your authority. This tool potential is to generate at least 5000 dollars daily. now check the flow all 5 stages, and see what is missing?? we need to provide users high quality videos, users should able to choose the script or video lenght, it should be easy to use, we should have all functions available to make awesome viral looking videos, how is the script, how is the hook? voice over narrator?? where they can select it, i mean check everything check the entire flow to make sure it takes them step by step to finish to publish the vidoe, check the editing studio, check the first and second and third stage? are we missing something, is all perfectly in flow?? make sure dont have any duplicates , we dont wont to confuse users, analize this and see what can be improved, users should start from simple prompt, and also users should able to start with brand identiy hub if they choose so, so that they will get similar videos for they channel, where they can select video lenght? where they can select video size?? i mean check that all is in order 100%, do not confuse users, it has to be best user experience, users should love the app, easy to use, with all the functions they need, it should be better than capcut or vidiq, users should able to generate animation or cartoon videos if they choose, they should able to make historical videos, or vlogs, i mean we need to give users more room to choose what kind of videos they wanna create. You understand ?? please make this work, analize and make sure we hit score 10/10 in every scale. We deploy on vercel.com for https://www.viralyzaier.com.\n\nAlway member that im deploying with vercel.com to website viralyzaier.com\n\nVialyzaier Application Workflow: A Deep Dive\nThe application is structured as a five-stage guided workflow designed to take a user from a raw video idea to a fully optimized, published piece of content. Each stage builds upon the last, leveraging AI and various services to streamline the creative process.\nStage 1: The Spark (Project Kickoff)\nGoal: To capture the initial video idea and create a project container.\nProcess:\nThe user begins in the ProjectKickoff.tsx component, which offers several strategic starting points:\nFrom a Topic (Core Path): The user enters a simple text prompt describing their video idea (e.g., \"A tutorial on how to make sourdough bread\").\nAnalyze Competitor: The user provides a URL to an existing YouTube video. The backend gemini-proxy function is invoked, which uses the Gemini model with Google Search grounding to analyze the competitor's video. It deconstructs the video's title, structure, and keywords, then suggests several \"upgraded\" titles. The user can select one of these titles to form the basis of their new project.\nExplore Trends: The user enters a broad niche (e.g., \"AI Productivity Tools\"). The backend again uses Gemini with Google Search to find recent, high-performing content in that niche. It synthesizes this research into three distinct, breakout video ideas, each with a suggested title, angle, and hook. The user can select one to create a new project.\nTechnical Actions:\nRegardless of the path chosen, a new project is created in the Supabase projects database table via a call to supabaseService.createProject.\nThe project is initialized with a status of 'Idea' and the workflowStep is set to 2.\nThe application state is updated, and the user is automatically navigated to the next stage.\nStage 2: The Blueprint (AI-Powered Strategy & Asset Generation)\nGoal: To transform the initial idea into a complete, actionable creative plan with all necessary pre-production assets.\nProcess:\nThis stage is handled by the ScriptGenerator.tsx component. Since a new project has no script yet, the user is presented with the \"Build Your Blueprint\" interface. Here, they provide the AI with creative direction:\nVideo Style: Defines the tone and format (e.g., 'High-Energy Viral', 'Cinematic Documentary').\nVideo Length & Size: Sets the target duration and aspect ratio (e.g., 60 seconds, 9:16 vertical).\nAI Narrator & Voice: The user can enable an AI voiceover and select a voice from a list of ElevenLabs presets or their own previously cloned voices.\nVisual Moodboard: The user can opt-in to have the AI generate a concept image for each scene in the script.\nTechnical Actions (handleGenerateBlueprint function):\nThis is a multi-step, automated process that orchestrates several backend services:\nScript & Plan Generation: A detailed prompt containing all user selections is sent to the gemini-proxy function. Gemini generates a complete Blueprint object, which includes:\nA strategic summary.\nSeveral viral title options.\nA full, scene-by-scene Script with timecodes, visual descriptions, voiceover text, and onScreenText.\nA list of moodboardDescription prompts, one for each scene, tailored to the chosen video style.\nMoodboard Image Generation & Upload:\nIf enabled, the system iterates through each moodboardDescription.\nFor each prompt, it calls the gemini-proxy function again, this time to the imagen-3.0-generate-002 model, to generate a base64-encoded image.\nThis image is converted to a Blob and uploaded to a dedicated folder in Supabase Storage (userId/projectId/moodboard_X.jpg).\nThe public URL of the uploaded image is then saved back into the corresponding scene in the script object.\nVoiceover Generation & Upload:\nIf a narrator is enabled, the system iterates through the voiceover text for each scene.\nIt calls the elevenlabs-proxy function, which securely sends the text and chosen voice ID to the ElevenLabs API to generate an MP3 audio file.\nThe returned audio Blob is uploaded to Supabase Storage (userId/projectId/voiceover_scene_X.mp3).\nThe public URL is saved to a voiceoverUrls map in the project data.\nSound Design Generation:\nThe AI is prompted to suggest a background music style and a list of sound effects (SFX) with specific timecodes based on the script's content.\nThe music suggestion is used to search the Jamendo music library via the jamendo-proxy. The top result is selected.\nEach SFX description is sent to the elevenlabs-proxy to generate an audio file, which is then uploaded to Supabase Storage.\nInitial Timeline Construction:\nThe buildTimelineFromProject utility function takes all the generated assets (images, voiceovers, music, SFX) and the script's timecodes.\nIt programmatically constructs a JSON object that conforms to the Shotstack Studio SDK's timeline schema. This JSON file defines all the tracks and places each asset as a clip at the correct time.\nFinal Update: All generated data—the script, asset URLs, and the Shotstack timeline JSON—is saved to the project record in Supabase. The workflowStep is updated to 3, automatically advancing the user to the Creative Studio.\nStage 3: The Creative Studio (Interactive Editing)\nGoal: To provide a professional, timeline-based editor for the user to review, refine, and finalize their video.\nProcess:\nThe CreativeStudio.tsx component is the core of this stage.\nTechnical Actions:\nSDK Initialization: The Shotstack Studio SDK is loaded.\nTimeline Loading:\nThe shotstackEditJson saved in the previous stage is retrieved from the active project's data.\nCrucially, all asset URLs within this JSON are passed through a proxy function (proxyifyEdit). This wraps each URL to point to our asset-proxy Supabase function. This is essential for two reasons: it bypasses potential CORS issues from asset hosts, and it uses a path-based URL structure (.../filename.mp3) that the editor's media loader relies on to correctly identify file types.\nThe processed timeline JSON is loaded into the editor, populating the timeline and preview canvas with all the AI-generated assets.\nInteractive Editing: The user has full control to:\nPlay, pause, and scrub the timeline.\nTrim, move, or delete clips.\nChange on-screen text content and styling.\nReplace visuals or audio by opening an AssetBrowserModal, which allows searching stock libraries (Pexels, Giphy, Jamendo) or generating new AI assets on the fly.\nAdjust clip properties like transitions and volume.\nState Synchronization: Every significant action in the editor triggers an edit:updated event. A listener captures the new timeline JSON and saves it back to Supabase in real-time, ensuring no work is lost.\nRendering: When the user clicks \"Render & Proceed,\" the handleRenderProject function is called.\nIt sends the final timeline JSON to our shotstack-render Supabase function.\nThis function acts as a secure proxy: it adds a callback URL (pointing to our shotstack-webhook function) to the JSON and forwards it to the Shotstack Render API.\nShotstack begins the cloud rendering process and returns a unique renderId.\nThe renderId is saved to the project, and the project status is set to 'Rendering', with workflowStep updated to 4.\nStage 4: Analysis & Report (Post-Render)\nGoal: To review the final rendered video and receive an AI-powered analysis of its viral potential.\nProcess:\nThe AnalysisStep.tsx component manages this stage.\nTechnical Actions:\nWaiting for Render: The UI displays a loader while the project status is 'Rendering'. In the background, the shotstack-webhook Supabase function is waiting. Once Shotstack finishes rendering, it calls this webhook. The webhook updates the project's status to 'Rendered' and saves the final finalVideoUrl.\nReal-time Update: The frontend, connected to Supabase's real-time updates, detects the status change and automatically moves to the next step.\nVirality Analysis: The analyzeVideoConcept service is called. It sends the project's original script and title to the gemini-proxy. Gemini performs a strategic analysis and returns a detailed Analysis object containing scores, a summary, strengths, and areas for improvement. This result is saved to the project.\nDisplay Results: The AnalysisResult.tsx component shows the final rendered video in a player alongside the complete virality report. The user can either proceed or return to the studio to make changes.\nProceeding: Clicking \"Proceed to Launchpad\" updates the workflowStep to 5.\nStage 5: The Launchpad (Publishing & Promotion)\nGoal: To prepare the final metadata, generate thumbnails, and publish the video directly to social platforms.\nProcess:\nThe Launchpad.tsx component is the final step.\nTechnical Actions:\nMetadata Generation: The user can click buttons to trigger various AI services via the gemini-proxy:\nGenerate SEO: Creates an optimized YouTube title, description, and tags.\nGenerate Thumbnails: First asks Gemini for two distinct thumbnail prompts, then calls the image generation model twice to create two visual options.\nDirect Publishing:\nIf the user has connected their YouTube account, the \"Publish\" button is active.\nClicking it invokes the youtube-publish Supabase function.\nThis secure backend function downloads the video file from the finalVideoUrl in Supabase Storage.\nIt uses the user's stored OAuth tokens to authenticate with the YouTube API and uploads the video file along with the title, description, tags, and selected thumbnail.\nUpon successful upload, YouTube returns the final video URL.\nThe project's status is updated to 'Published', and the publishedUrl is saved.\nScheduling: Alternatively, the user can schedule the video for a later date, which updates the status to 'Scheduled' and saves the date to the project.\n",
  "requestFramePermissions": [
    "microphone"
  ],
  "prompt": ""
}